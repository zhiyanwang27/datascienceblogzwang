{
  
    
        "post0": {
            "title": "Hypothesis Testing and Credibility Crisis in Psychology Studies",
            "content": "Replication Crisis . In recent years, there have been great concerns about the replicability of a great number of social psychology studies. One of the famous examples is the study about &quot;elderly related word prime slow walking&quot;. In the original study published in 1996, the researchers reported that when people read elderly related words compared to neutral words, the speed of their walking is slower. Many attempts have been conducted to replicate the study. However, these attempts have failed. This is just one example of the replication crisis. In the 21st century, a lot of these surprising social psychology findings that are usually found in textbooks cannot be replicated. Researchers have then started to think about these issues. One of the problems with the previous findings is the &quot;crazy&quot; search for the gold standard p&lt;.05. . Sample size inflation . Implicitly and explicitly, researchers seem to agree that in order for a study to be published it&#39;s necessary to acquire a significance level of p&lt;.05. From our knowledge about hypothesis testing, we know that p-value can be influenced by a number of different factors. One of the factors is the sample size. If we have a very large sample size, it&#39;s easier to get a p-value less than 0.05 even though the effect itself is really small. Therefore, if we only care about the p-value, we can try to acquire our &quot;gold standard&quot; p&lt;.05 by inflating our sample sizes. The intentional inflating of sample sizes might generate difficulty for later researchers who attempt to replicate the study with a smaller sample size. . Therefore, to better accommodate this issue. Recent studies have focused on a scientific justification of sample sizes as well as a detailed reporting of other aspects of the samples as compared to just the p-value. For example, people may report individual data as well as the effect size. . P-hacking and Multiple Testing . Another issue is referred to as p-hacking. P-hacking is defined as the process in which the researchers test many combinations of observations from the same problem for a number of times until one of the results reach the desired significance which is p&lt;.05. The researchers in the end only reported the only signficiant report assuming that such statistical testing results were obtained with a legitimate and reasonable hypothesis testing procedure. One of the problems associated with p-hacking is multiple testing. If imagine for one test we set the alpha value to be 0.05, what would be the probability if you perform 10 similar tests and have one false rejection of the null hypothesis. This can be computed as $1-0.95^10$. If we calcualte the value, it&#39;s 0.4!. This is surprisingly large and almost 10 times as the alpha value we set. This means that the probability we are making a false rejection of the null hypothesis becomes 0.4 if have multiple testing. Now, Let&#39;s rethink about to the p-hacking processes. It&#39;s easy to see where the problem is coming from. If we test the same problem a number of times with the hope to achieve p&lt;.05, we are actually making multiple tests. Even though we found a significant test at the p&lt;.05 level individually, the overall p-value for the question we are interesting in is much larger than that. We are most likely making a conclusion based on a false rejection of the null hypothesis. Consequently, for later researches who would like to replicate the study, they cannot achieve the same p-value if they only test the dataset for once. Multiple testing can be corrected with Bonferroni correction in which the alpha value is divided by the number of tests that were performed. . A recent publich in Nature focuses on the issue as well. The study in Nature observed the results from 70 research teams that were assigned the same brain imaging dataset. Some teams were aware of the hypotheses that they were supposed to test while others were just exploring the dataset freely with their own hypotheses. Surprising, the result obtained by the 70 teams were fundamentally different from each other even though they were assigned the same dataset. One of the factors that affected the issue is difference in multiple testing. The teams that were aware of the hypothese might be specifically looking for certain results. They might adjust their interested brain region multiple tests until a desired result were acquired. Moreover, even when researchers were not aware of the hypotheses, they may have arbitrary criterions for multiple testing corrections. . Moving forward . One of the promising aspects of the replication crisis is the movment towards open science. A number of efforts have been made to make the testing processes as well as the reporting processes more transparent to promote replicability. The measures that have been taken include the following: . Reporting individual observations | Reporting effect sizes and justification for sample sizes | Reporting justification for statitical methods and multiple correction methods | Publishing analysis codes and datasets | Pre-register study with hypothesis, sample sizes, statitical methods to avoid p-hacking | . References . Botvinik-Nezer, R., Holzmeister, F., Camerer, C.F. et al. Variability in the analysis of a single neuroimaging dataset by many teams. Nature 582, 84–88 (2020). https://doi.org/10.1038/s41586-020-2314-9 .",
            "url": "https://zhiyanwang27.github.io/datascienceblogzwang/jupyter/2020/11/18/discussion.html",
            "relUrl": "/jupyter/2020/11/18/discussion.html",
            "date": " • Nov 18, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Introduction to Hypothesis Testing",
            "content": "Concepts . What is hypothesis testing and why do we need it . Hypothesis testing is an important concept in statistics. It is a framework in which we evaluate whether a given set of observations are consistent with a specific hypothesis. The hypothesis, in most cases, is about the underlying distribution from which the observed data is drawn from. Hypothesis testing usually make predictions about the following scenarios:whether the observations come from a certain distribution with a mean and standard deviation, whether two groups of observations come from the same underlying distributions etc. Why do we need hypothesis testing ? In most real-life datasets that we observed, we cannot have access to the underlying distribution of the data we obtained from. However, we often make statements (hypothesis) with regard to the underlying properties within the dataset. Although the processes of making hypotheses sound distant and abstract, it&#39;s actually quite common for us to make a hypothesis in real life about a set of observed data. For example, imagine if we want to evaluate the effect of a new medicine treating blood pressure. We observe that on average the group of patients who took the medicine have a blood pressure of 138 while the group of patients who did not take the medicine have a blood pressure of 144. From a naive point of view, we would say the medicine is effective as the medicine group had a lower blood pressure. The statement we made about the difference group is a hypothesis. How can we say this hypothesis true? How likely should we believe this hypothesis? Therefore, we need to use a systematic way to evaluate our hypotheses. . Hypothesis testing involves the following procedures: Suppose we have a number of observations $X_1,X_2,X_3.,...,X_n$ from a distribution. . We construct a null hypothesis $H_0$ about the underlying distribution. | We compute a test statsitic T. Assuming that $H_0$ is true, T is a function of the observations X for which we can evalute the distribution of T. | We give an alternative hypothesis $H_a$. Under the alternative hypothesis, the distribution of T will be significantly different than under $H_0$. | We choose a signficance level $ alpha$ (usually 5%). Based on $H_a$, we define a set of values of T as the critical region in which with at most probability $ alpha$ we would be able to observe if $H_0$ is true. | We obtained the empirical t and evaluate whether t falls under the critical region. If t is in the critical region we will reject the null hypothesis. We will also obtain a p-value. A p-value is the probability that the observations can be obtained if the null hypothesis is true. P-value is also the minimal $ alpha$ value which we can reject the null hypothesis. | . The graph below illustrates the idea of p-values. The two hypotheses are plotted as normal distributions. The empirical t is calculated based on $H_a$. However p-value is obtained with $H_0$. We evaluate the probability of getting t under $H_0$. In the graph, this is the region right of the green line under the blue curve ($H_0$) . Hypothesis testing involves parametric and non-parametric testing. Parametric testing makes hypothesis about a population parameter such as the mean. Non-parametric testing does not make hypothesis about the population parameter. . The most commonly used parametric tests is the t test. The T statistics can be calcualted as follows: . For comparison against a population mean: $t = frac{ bar{X} - mu}{S/ sqrt{n}}$ whereas $ bar{X}, S,n$ refers to the sample mean, sample variance and the sample size. For comparison between two means: $t = frac{ bar{X_1} - bar{X_2}}{ sqrt{s1^2/n_1 + s2^2/n_2}}$ whereas $ bar{X_1}, bar{X_2},s_1,s_2,n_1,n_2$ refers to the two sample means, two sample variances and the two sample sizes. . When we use t test, the test statsitic follows a t distribution which is conditioned on the degrees of freedom. The larger the sample size, the close the t distribution approches a normal distribution. . . A toy example . Understand hypothesis testing in a toy example . In this example, we explore the idea of hypothesis testing with a toy example. Let&#39;s continue the idea of the blood pressure example. Suppose that for the group with medication the blood pressure is 138 140 141 138 133 138. The placebo group without medication has blood pressure 144 134 145 154 139 149. Just looking at the data, we cannot make a conclusion whether the medication is effective. . First, let&#39;s construct the $H_0$,$H_a$,$T$,$ alpha$. The null hypothesis that the group mean is not different $ mu_1= mu_2$. The alternative hypothesis is $ mu_1 neq mu_2$. We calculate T with the two group formula above. We define $ alpha$ to be 0.05. | Second, we compute the empirical t with the data | Third, we acquire the p-value with the current degree of freedom. | Finally, we make a conclusion about the dataset. | . After the calculation, we obtained a pvalue of 0.045. This falls with the critical range we defined. Therefore we reject the hypothesis that the means of the two groups are the same and can conclude the medicine is effective. Expand the code below to see how this is done. . using Statistics; patient = [138,140,141,138,133,138]; placebo = [144,134,145,154,139,149]; m₁, m₂ = mean(patient), mean(placebo); s₁, s₂ = std(patient), std(placebo); n₁, n₂ = length(patient), length(placebo); . . a = s₁^2/n₁; b = s₂^2/n₂; ν = (a + b)^2 / (a^2/(n₁-1) + b^2/(n₂-1)); t = (m₁ - m₂)/sqrt(a+b); 1-ccdf(TDist(ν), t); . . A real world example . Understand hypothesis testing with Covid-19 treatment Remdesivir . A new medicine has been developed with the aim to treat Covid-19 which is called Remdesivir. A number of results have been released to evalute the effects of Remdesivir on Covid-19 patients. Hypothesis testing is widely used when we interpretting the results. The following data is from the announcement of the Remdesivir clinical trials . Let&#39;s take a look one of the results of the clinical trials. Covid19 patients received either placebo for 10 days or intravenous (IV) remdesivir at a dose of 200 mg on Day 1 and then 100 mg daily for up to 9 more days. The study examined 1,062 participants. One of the results looked at the time to recovery compared to placebo. The median time to recovery was 10 days for Remdesivir group whereas the median time to recover was 15 days. They reported a p value less than 0.001. Based on the hypothesis testing knowledge we discussed above, we can say based on the testing results, we can reject the null hypohtesis that the placebo and the medicine group has the same median recovery time. The likelihood of getting these observations under the null hypothesis is less than 0.001. This is a very low p value. We can preliminarily conclude the medicine reduces recovery time. . Let&#39;s look at another example with the same medicine in a different patient sample. This study evaluated patients with severe COVID-19 in China. Patients were randomized 2:1 to receive IV remdesivir (200 mg on Day 1, followed by 100 mg daily) or normal saline placebo for 10 days. 237 patients were randomized to receive remdesivir (n = 158) or placebo (n = 79). However, they found no difference in the time to clinical improvement, 21 days to 23 days. Numerically, these days look different, but statistically we cannot say there is a significant difference between the groups. . If you are interested in the study, feel free to read the whole report with the idea of hypothesis testing in mind. . We need to keep one thing in mind, interpreting hypothesis testing is always tricky. See the next posts about the discussion of hypothesis testing. .",
            "url": "https://zhiyanwang27.github.io/datascienceblogzwang/jupyter/2020/11/15/main.html",
            "relUrl": "/jupyter/2020/11/15/main.html",
            "date": " • Nov 15, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://zhiyanwang27.github.io/datascienceblogzwang/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://zhiyanwang27.github.io/datascienceblogzwang/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}