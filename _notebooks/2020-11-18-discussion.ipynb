{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing and Credibility Crisis in Psychology Studies\n",
    "> A brief discussion about hypothesis testing and the replication crisis related to hypothesis testing\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: images/chart-preview.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replication Crisis\n",
    "\n",
    "In recent years, there have been great concerns about the replicability of a great number of social psychology studies. One of the famous examples is the study about [\"elderly related word prime slow walking\"](http://www.psychfiledrawer.org/replication.php?attempt=MTU%3D). In the original study published in 1996, the researchers reported that when people read elderly related words compared to neutral words, the speed of their walking is slower. Many attempts have been conducted to replicate the study. However, these attempts have failed. This is just one example of the replication crisis. In the 21st century, a lot of these surprising social psychology findings that are usually found in textbooks cannot be replicated. Researchers have then started to think about these issues. One of the problems with the previous findings is the \"crazy\" search for the gold standard p<.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample size inflation\n",
    "\n",
    "Implicitly and explicitly, researchers seem to agree that in order for a study to be published it's necessary to acquire a significance level of p<.05. From our knowledge about hypothesis testing, we know that p-value can be influenced by a number of different factors. One of the factors is the sample size. If we have a very large sample size, it's easier to get a p-value less than 0.05 even though the effect itself is really small. Therefore, if we only care about the p-value, we can try to acquire our \"gold standard\" p<.05 by inflating our sample sizes. The intentional inflating of sample sizes might generate difficulty for later researchers who attempt to replicate the study with a smaller sample size.   \n",
    "\n",
    "Therefore, to better accommodate this issue. Recent studies have focused on a scientific justification of sample sizes as well as a detailed reporting of other aspects of the samples as compared to just the p-value. For example, people may report individual data as well as the effect size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-hacking and Multiple Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another issue is referred to as p-hacking. P-hacking is defined as the process in which the researchers test many combinations of observations from the same problem for a number of times until one of the results reach the desired significance which is p<.05. The researchers in the end only reported the only signficiant report assuming that such statistical testing results were obtained with a legitimate and reasonable hypothesis testing procedure. One of the problems associated with p-hacking is multiple testing. If imagine for one test we set the alpha value to be 0.05, what would be the probability if you perform 10 similar tests and have one false rejection of the null hypothesis. This can be computed as $1-0.95^10$. If we calcualte the value, it's 0.4!. This is surprisingly large and almost 10 times as the alpha value we set. This means that the probability we are making a false rejection of the null hypothesis becomes 0.4 if have multiple testing. Now, Let's rethink about to the p-hacking processes. It's easy to see where the problem is coming from. If we test the same problem a number of times with the hope to achieve p<.05, we are actually making multiple tests. Even though we found a significant test at the p<.05 level individually, the overall p-value for the question we are interesting in is much larger than that. We are most likely making a conclusion based on a false rejection of the null hypothesis. Consequently, for later researches who would like to replicate the study, they cannot achieve the same p-value if they only test the dataset for once. Multiple testing can be corrected with Bonferroni correction in which the alpha value is divided by the number of tests that were performed. \n",
    "\n",
    "A recent publich in [Nature](https://www.nature.com/articles/s41586-020-2314-9) focuses on the issue as well. The study in Nature observed the results from 70 research teams that were assigned the same brain imaging dataset. Some teams were aware of the hypotheses that they were supposed to test while others were just exploring the dataset freely with their own hypotheses. Surprising, the result obtained by the 70 teams were fundamentally different from each other even though they were assigned the same dataset. One of the factors that affected the issue is difference in multiple testing. The teams that were aware of the hypothese might be specifically looking for certain results. They might adjust their interested brain region multiple tests until a desired result were acquired. Moreover, even when researchers were not aware of the hypotheses, they may have arbitrary criterions for multiple testing corrections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving forward\n",
    "One of the promising aspects of the replication crisis is the movment towards open science. A number of efforts have been made to make the testing processes as well as the reporting processes more transparent to promote replicability. The measures that have been taken include the following: \n",
    "* Reporting individual observations\n",
    "* Reporting effect sizes and justification for sample sizes\n",
    "* Reporting justification for statitical methods and multiple correction methods\n",
    "* Publishing analysis codes and datasets\n",
    "* Pre-register study with hypothesis, sample sizes, statitical methods to avoid p-hacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Botvinik-Nezer, R., Holzmeister, F., Camerer, C.F. et al. Variability in the analysis of a single neuroimaging dataset by many teams. Nature 582, 84â€“88 (2020). https://doi.org/10.1038/s41586-020-2314-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
